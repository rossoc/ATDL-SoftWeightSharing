{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlorosso/Documents/ATDL-SoftWeightSharing/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from models.lenet import LeNet\n",
    "from models.utils import PyTorchNeuralNetworkWrapper\n",
    "from sws_compressor import SoftWeightSharingCompressor\n",
    "from dataset import get_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pretraining and compression process...\n",
      "Loading MNIST dataset...\n",
      "Training data shape: (60000, 784)\n",
      "Test data shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting pretraining and compression process...\")\n",
    "# Load MNIST dataset\n",
    "print(\"Loading MNIST dataset...\")\n",
    "X_train, X_test, y_train, y_test = get_mnist()\n",
    "\n",
    "# Use a smaller subset for faster training\n",
    "X_train = X_train\n",
    "y_train = y_train\n",
    "X_test = X_test\n",
    "y_test = y_test\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LeNet model...\n",
      "Creating sklearn-compatible wrapper...\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LeNet model\n",
    "print(\"Initializing LeNet model...\")\n",
    "model = LeNet()\n",
    "\n",
    "# Wrap the model with the sklearn-compatible wrapper\n",
    "print(\"Creating sklearn-compatible wrapper...\")\n",
    "clf = PyTorchNeuralNetworkWrapper(\n",
    "    model=model,\n",
    "    epochs=20,  # Fewer epochs for testing\n",
    "    batch_size=64,\n",
    "    learning_rate=5e-4,  # Standard learning rate\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting pretraining...\")\n",
    "# Pretrain the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the pretrained model\n",
    "train_score = clf.score(X_train, y_train)\n",
    "test_score = clf.score(X_test, y_test)\n",
    "print(f\"Pretrained model - Train accuracy: {train_score:.4f}\")\n",
    "print(f\"Pretrained model - Test accuracy: {test_score:.4f}\")\n",
    "\n",
    "# Save the pretrained model\n",
    "pretrained_model_path = \"pretrained_lenet_model.pth\"\n",
    "clf.save_model(pretrained_model_path)\n",
    "print(f\"Pretrained model saved to {pretrained_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Now, let's compress the model using SWS\n",
    "print(\"Starting SWS compression...\")\n",
    "\n",
    "# Create a new instance of the model to load the pretrained weights\n",
    "compressed_model = LeNet()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "compressed_model = compressed_model.to(device)\n",
    "\n",
    "# Load the pretrained weights\n",
    "model_data = torch.load(pretrained_model_path, map_location=device, weights_only=False)\n",
    "compressed_model.load_state_dict(model_data[\"model_state_dict\"])\n",
    "compressed_model.eval()\n",
    "\n",
    "# Prepare test data for compression (we need a DataLoader)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test).to(device)\n",
    "\n",
    "# Reshape if needed (for image data)\n",
    "if len(X_test_tensor.shape) == 2 and X_test_tensor.shape[1] == 784:  # Likely flattened MNIST\n",
    "    batch_size = X_test_tensor.size(0)\n",
    "    X_test_tensor = X_test_tensor.view(batch_size, 1, 28, 28)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the SWS compressor\n",
    "# Using random values for the gamma inverse distribution and Beta priors\n",
    "sws_compressor = SoftWeightSharingCompressor(\n",
    "    tau=5e-3,\n",
    "    n_components=17,\n",
    "    zero_component_prior_alpha=2.0,  # Beta(2.0, 2.0) promotes reasonable zero component weight\n",
    "    zero_component_prior_beta=2.0,   # Beta(2.0, 2.0) promotes reasonable zero component weight\n",
    "    other_var_prior_alpha=np.random.uniform(0.5, 2.0),  # Random gamma inv dist values\n",
    "    other_var_prior_beta=np.random.uniform(0.5, 2.0),   # Random gamma inv dist values\n",
    "    lr_weights=1e-4,\n",
    "    lr_mixture=5e-4,\n",
    "    prune_threshold=1e-4,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Compressing model with SWS...\")\n",
    "# Apply compression\n",
    "compressed_state_dict, code_book, mixture_params = sws_compressor(compressed_model, test_loader, epochs=10)\n",
    "\n",
    "# Create a Prior object using the final mixture parameters\n",
    "prior = Prior(mixture_params['mu'], mixture_params['sigma'], mixture_params['pi'])\n",
    "\n",
    "# Load the compressed weights back into the model\n",
    "compressed_model.load_state_dict(compressed_state_dict)\n",
    "\n",
    "# Evaluate the compressed model\n",
    "compressed_model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_final = torch.FloatTensor(X_test).to(device)\n",
    "    if len(X_test_final.shape) == 2 and X_test_final.shape[1] == 784:\n",
    "        batch_size = X_test_final.size(0)\n",
    "        X_test_final = X_test_final.view(batch_size, 1, 28, 28)\n",
    "    \n",
    "    outputs = compressed_model(X_test_final)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    compressed_accuracy = (predicted.cpu().numpy() == y_test).mean()\n",
    "\n",
    "print(f\"Compressed model - Test accuracy: {compressed_accuracy:.4f}\")\n",
    "\n",
    "# Calculate compression statistics\n",
    "original_params = sum(p.numel() for p in model.parameters())\n",
    "unique_weights = len(torch.unique(torch.cat([p.flatten() for p in model.parameters()])))\n",
    "compressed_params = len(code_book['centres'])\n",
    "\n",
    "print(f\"\\nCompression Results:\")\n",
    "print(f\"Original number of parameters: {original_params}\")\n",
    "print(f\"Number of unique weights after compression: {compressed_params}\")\n",
    "print(f\"Compression ratio: {original_params / len(code_book['centres']):.2f}x\")\n",
    "print(f\"Accuracy drop: {test_score - compressed_accuracy:.4f}\")\n",
    "\n",
    "# Generate detailed compression report using the new function\n",
    "print(\"\\nGenerating detailed compression report...\")\n",
    "compression_details = compression_report(\n",
    "    model=compressed_model,  # Use the compressed model\n",
    "    prior=prior,\n",
    "    dataset=\"MNIST\",  # Dataset name\n",
    "    use_huffman=True,\n",
    "    pbits_fc=5,\n",
    "    pbits_conv=8,\n",
    "    skip_last_matrix=False,\n",
    "    assign_mode=\"ml\"  # Should match the quantization method used in compression\n",
    ")\n",
    "\n",
    "print(f\"Detailed Compression Report:\")\n",
    "print(f\"  Original bits: {compression_details['orig_bits']:,}\")\n",
    "print(f\"  Compressed bits: {compression_details['compressed_bits']:,}\")\n",
    "print(f\"  Compression Ratio: {compression_details['CR']:.2f}x\")\n",
    "print(f\"  Non-zero elements: {compression_details['nnz']:,}\")\n",
    "print(f\"  Layer-by-layer breakdown:\")\n",
    "for layer_info in compression_details['layers']:\n",
    "    print(f\"    {layer_info['layer']} {layer_info['shape']}: {layer_info['orig_bits']} -> {layer_info['bits_IR'] + layer_info['bits_IC'] + layer_info['bits_A'] + layer_info['bits_codebook']} bits\")\n",
    "\n",
    "# Save the compressed model\n",
    "compressed_model_path = \"compressed_lenet_model.pth\"\n",
    "torch.save({\n",
    "    'state_dict': compressed_state_dict,\n",
    "    'code_book': code_book,\n",
    "    'original_accuracy': test_score,\n",
    "    'compressed_accuracy': compressed_accuracy\n",
    "}, compressed_model_path)\n",
    "print(f\"Compressed model saved to {compressed_model_path}\")\n",
    "\n",
    "print(\"Process completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pretraining and compression process...\n",
      "Loading MNIST dataset...\n",
      "Training data shape: (5000, 784)\n",
      "Test data shape: (1000, 784)\n",
      "Initializing LeNet model...\n",
      "Creating sklearn-compatible wrapper...\n",
      "Starting pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:05<00:00,  1.14s/it, loss=0.0406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model - Train accuracy: 0.9962\n",
      "Pretrained model - Test accuracy: 0.9510\n",
      "Pretrained model saved to pretrained_lenet_model.pth\n",
      "Starting SWS compression...\n",
      "Compressing model with SWS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retraining: 100%|██████████| 10/10 [00:09<00:00,  1.02it/s, Epoch=10, $\\mathcal{L}_E$=0.109, $\\mathcal{L}_C$=-7.5e+5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed model - Test accuracy: 0.0850\n",
      "\n",
      "Compression Results:\n",
      "Original number of parameters: 431080\n",
      "Number of unique weights after compression: 8\n",
      "Compression ratio: 53885.00x\n",
      "Accuracy drop: 0.8660\n",
      "Compressed model saved to compressed_lenet_model.pth\n",
      "Process completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdQtWoC_sQWh"
   },
   "source": [
    "# LeNet-300-100 Experiment\n",
    "## The exact same baseline as in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J7N1l8v8bSGt",
    "outputId": "21bdc44e-8c7a-4076-dd72-118c26e372d5"
   },
   "outputs": [],
   "source": [
    "!python run_sws.py --preset lenet_300_100 \\\n",
    "  --complexity-mode keras --tau 5e-3 \\\n",
    "  --quant-assign map \\\n",
    "  --auto-tau-ratio 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRpU1_7TvDzn"
   },
   "source": [
    "# LeNet-300-100 (ours robust ML-assignment + automatic $\\tau$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aD5ntaGOufGe",
    "outputId": "a8ec6836-ef7c-4905-e497-07f60c127738"
   },
   "outputs": [],
   "source": [
    "!python run_sws.py --preset lenet_300_100 \\\n",
    "  --pretrain-epochs 30 --retrain-epochs 30 \\\n",
    "  --pi0 0.95 --num-components 17 \\\n",
    "  --lr-w 5e-4 --lr-theta-means 1e-4 --lr-theta-gammas 3e-3 --lr-theta-rhos 3e-3 \\\n",
    "  --weight-decay 0.0 \\\n",
    "  --complexity-mode epoch --tau 3e-5 --tau-warmup-epochs 5 \\\n",
    "  --gamma-alpha 50 --gamma-beta 0.1 \\\n",
    "  --gamma-alpha-zero 100 --gamma-beta-zero 0.5 \\\n",
    "  --merge-kl-thresh 0.0 --quant-skip-last \\\n",
    "  --quant-assign ml \\\n",
    "  --log-mixture-every 1 --cr-every 5 \\\n",
    "  --run-name pt_lenet300_ml --save-dir runs --seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdLF_jpyOg3k"
   },
   "source": [
    "# LeNet-Caffe experiment\n",
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-393pqikepxa",
    "outputId": "4f36c99e-846a-4250-e285-031a42e97b56"
   },
   "outputs": [],
   "source": [
    "!python run_sws.py --preset lenet5 \\\n",
    "  --complexity-mode keras --tau 5e-3 \\\n",
    "  --quant-assign map \\\n",
    "  --auto-tau-ratio 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t71yB2ktu5Qo"
   },
   "source": [
    "# LeNet-Caffe (ours robust ML-assignment + automatic $\\tau$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UlmlGDOsOi1_",
    "outputId": "79a1c0f2-c21b-489e-a9da-efa53021a517"
   },
   "outputs": [],
   "source": [
    "!python run_sws.py --preset lenet5 \\\n",
    "  --pretrain-epochs 100 --retrain-epochs 60 \\\n",
    "  --pi0 0.95 --num-components 17 \\\n",
    "  --lr-w 5e-4 --lr-theta-means 1e-4 --lr-theta-gammas 3e-3 --lr-theta-rhos 3e-3 \\\n",
    "  --weight-decay 0.0 \\\n",
    "  --complexity-mode epoch --tau 3e-5 --tau-warmup-epochs 5 \\\n",
    "  --gamma-alpha 50 --gamma-beta 0.1 \\\n",
    "  --gamma-alpha-zero 100 --gamma-beta-zero 0.5 \\\n",
    "  --merge-kl-thresh 0.0 --quant-skip-last \\\n",
    "  --quant-assign ml \\\n",
    "  --log-mixture-every 1 --cr-every 5 \\\n",
    "  --run-name pt_lenet5_ml_safe --save-dir runs --seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INc4lACKRZ70"
   },
   "source": [
    "# ResNet (light) Experiment\n",
    "## The same as in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K--XzrKivKd6",
    "outputId": "abd53cd5-acd4-431c-d374-78ee19a052cc"
   },
   "outputs": [],
   "source": [
    "!python run_sws.py --preset wrn_16_4 \\\n",
    "  --complexity-mode keras --tau 5e-3 \\\n",
    "  --quant-assign map --auto-tau-ratio 0 \\\n",
    "  --log-mixture-every 1 --make-gif \\\n",
    "  --run-name wrn_map_keras_tau5e-3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRFc94RVvMvt"
   },
   "source": [
    "# ResNet (light) (ours robust ML-assignment + automatic $\\tau$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6o_JIJ76WhHt",
    "outputId": "236615b5-1bd3-44a8-f4ca-2fcbe00b22e5"
   },
   "outputs": [],
   "source": [
    "!python run_sws.py --preset wrn_16_4 \\\n",
    "  --complexity-mode epoch --auto-tau-ratio 0.1 \\\n",
    "  --tau-warmup-epochs 10 \\\n",
    "  --quant-assign ml --merge-kl-thresh 1e-6 \\\n",
    "  --log-mixture-every 1 --cr-every 2 \\\n",
    "  --make-gif --gif-fps 2 --gif-sample 50000 \\\n",
    "  --run-name wrn_ml_autoTau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKnt2JIlk0uJ"
   },
   "source": [
    "# Collapse demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LqNH07r8k1xR",
    "outputId": "3d9deca6-ae42-44d5-df08-375e2d764cd6"
   },
   "outputs": [],
   "source": [
    "!python run_sws.py --preset lenet_300_100 \\\n",
    "  --complexity-mode keras --tau 5e-3 \\\n",
    "  --quant-assign map --auto-tau-ratio 0 \\\n",
    "  --log-mixture-every 1 \\\n",
    "  --make-gif --gif-fps 2 --gif-sample 50000 \\\n",
    "  --gif-xmin -1.2 --gif-xmax 1.2 --gif-ymin -1.2 --gif-ymax 1.2 \\\n",
    "  --run-name lenet300_map_keras_tau5e-3_gif\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nd1JRLmspQuA"
   },
   "source": [
    "## Ours Robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vlnle8O9pR3I",
    "outputId": "59a07cea-b420-4195-d4f7-710ed7d2b2e4"
   },
   "outputs": [],
   "source": [
    "!python run_sws.py --preset lenet_300_100 \\\n",
    "  --complexity-mode epoch --auto-tau-ratio 0.1 \\\n",
    "  --tau-warmup-epochs 10 \\\n",
    "  --quant-assign ml --merge-kl-thresh 1e-6 \\\n",
    "  --log-mixture-every 1 \\\n",
    "  --make-gif --gif-fps 2 --gif-sample 50000 \\\n",
    "  --gif-xmin -1.2 --gif-xmax 1.2 --gif-ymin -1.2 --gif-ymax 1.2 \\\n",
    "  --run-name lenet300_ml_autoTau_gif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UsP11_I4tTXu",
    "outputId": "29d3a7c9-797a-42e9-b4eb-cf293a9a3247"
   },
   "outputs": [],
   "source": [
    "!python run_sws.py --preset lenet_300_100 \\\n",
    "  --complexity-mode epoch --tau 8e-5 --tau-warmup-epochs 10 \\\n",
    "  --quant-assign ml --merge-kl-thresh 1e-6 \\\n",
    "  --log-mixture-every 1 \\\n",
    "  --make-gif --gif-fps 2 --gif-sample 50000 \\\n",
    "  --gif-xmin -1.2 --gif-xmax 1.2 --gif-ymin -1.2 --gif-ymax 1.2 \\\n",
    "  --run-name banded_ml_tau8e-5\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial-softweightsharingfornncompression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
